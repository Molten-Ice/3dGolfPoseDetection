{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0XZeo5KtUn8785Xe5Nd0x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Molten-Ice/3dGolfPoseEstimation/blob/main/nb1_b_sortxyz_multiple_dataset_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set \"Repository secrets\" as environment variables\n",
        "import os\n",
        "os.environ['OPENAI_APIKEY'] = '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'\n",
        "os.environ['HF_TOKEN'] = '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'"
      ],
      "metadata": {
        "id": "JwsS6d_FSEXp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3l_2-glDmUj",
        "outputId": "54fc719e-41d2-43ec-95ff-f2d353154089"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import openai\n",
        "import textwrap\n",
        "import gradio as gr\n",
        "import huggingface_hub\n",
        "\n",
        "\n",
        "# HF api key has to be added in \"Repository secrets\" for this line to work\n",
        "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
        "OPENAI_APIKEY = os.environ.get(\"OPENAI_APIKEY\")\n",
        "huggingface_hub.login(HF_TOKEN)\n",
        "api = huggingface_hub.HfApi()\n",
        "# Setting paid openai key\n",
        "openai.api_key = OPENAI_APIKEY"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ho_LO3JD-wW",
        "outputId": "b9bf66e5-3fd5-4a0b-9345-7abbcb4ee119"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load input dataset\n",
        "# NOTE: first 5 prompts have be reserved for style generation\n",
        "# If these are used as inputs to chatgpt it will break\n",
        "with open('codepen_data.jsonl', 'r') as fp:\n",
        "    codepen_data = [json.loads(x) for x in fp.readlines()]\n",
        "\n",
        "def reformat_row(row):\n",
        "    \"\"\"Takes in:\n",
        "    {'id': 979, 'attributes': [{'value': 'Chillbucks Apron', 'trait_type': 'Accessories'}, {'value': 'Black Shirt', 'trait_type': 'Apparel'}, {'value': 'Blue', 'trait_type': 'Background'}, {'value': 'Uhhh', 'trait_type': 'Expression'}, {'value': 'Short Curly Black', 'trait_type': 'Hair'}, {'value': 'Purple', 'trait_type': 'Skin'}]}\n",
        "    Returns:\n",
        "    {'id': 979, 'Accessories': 'Chillbucks Apron', 'Apparel': 'Black Shirt', 'Background': 'Blue', 'Expression': 'Uhhh', 'Hair': 'Short Curly Black', 'Skin': 'Purple'}\n",
        "    length 329 -> len 163\n",
        "    \"\"\"\n",
        "    return {**{'id': row['id']}, **{pair['trait_type'] : pair['value'] for pair in row['attributes']}}\n",
        "\n",
        "print(codepen_data[0])\n",
        "formatted_data = [reformat_row(data_dict) for data_dict in codepen_data]\n",
        "for row in formatted_data[:3]:\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkZG5uym7z__",
        "outputId": "98e4ba16-60fa-4903-85f5-8f230d05a7e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 985, 'attributes': [{'value': 'Brown Aviator Sunglasses', 'trait_type': 'Accessories'}, {'value': 'Tangerine Button Up', 'trait_type': 'Apparel'}, {'value': 'Purple', 'trait_type': 'Background'}, {'value': 'Teeth', 'trait_type': 'Expression'}, {'value': 'Black Helmet Head', 'trait_type': 'Hair'}, {'value': 'Green', 'trait_type': 'Skin'}]}\n",
            "{'id': 985, 'Accessories': 'Brown Aviator Sunglasses', 'Apparel': 'Tangerine Button Up', 'Background': 'Purple', 'Expression': 'Teeth', 'Hair': 'Black Helmet Head', 'Skin': 'Green'}\n",
            "{'id': 987, 'Accessories': 'Candy Beads Necklace', 'Apparel': 'Black Jean Jacket & Striped Shirt', 'Background': 'Pink', 'Expression': 'Pouty', 'Hair': 'Long Wavy Pastel & Black Headband', 'Skin': 'Yellow'}\n",
            "{'id': 984, 'Apparel': 'Purple Tie-Dye Shirt & Backpack', 'Background': 'Green', 'Expression': 'Smile', 'Facial Features': 'Stuble', 'Hair': 'Hova', 'Skin': 'Red'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Currently using background 1 for all predictions\n",
        "chillennials_background1 = \"Screwed, Broke, and still Chill AF — Chillennials are a 20K PFP collection with college debt, net worths, jobs, stress, and boomer parents who can barely use the internet and constantly call us for tech support. F***CK Mom! TRY GOOGLING!\"\n",
        "chillennials_background2 = \"About The Project: The Chillennials are a generation laden with debt, rising inflation, a climate crisis, a housing shortage, an economic shutdown, and a global pandemic. We aren't defined by an age cohort, but by the fact that the Boomers traded our futures away for a few extra cucks. Yet here we are, chill AF, and in the most ironic turn of events, it's our ability (and their inability) to use the internet that will usher in the greatest transfer of wealth in human history. WAGMI.\"\n",
        "print(textwrap.fill(chillennials_background1, width=100), \"\\n\")\n",
        "print(textwrap.fill(chillennials_background2, width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfcH2aMVBIfl",
        "outputId": "a02c4b9f-f245-49a2-9d04-a58042e516c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Screwed, Broke, and still Chill AF — Chillennials are a 20K PFP collection with college debt, net\n",
            "worths, jobs, stress, and boomer parents who can barely use the internet and constantly call us for\n",
            "tech support. F***CK Mom! TRY GOOGLING! \n",
            "\n",
            "About The Project: The Chillennials are a generation laden with debt, rising inflation, a climate\n",
            "crisis, a housing shortage, an economic shutdown, and a global pandemic. We aren't defined by an age\n",
            "cohort, but by the fact that the Boomers traded our futures away for a few extra cucks. Yet here we\n",
            "are, chill AF, and in the most ironic turn of events, it's our ability (and their inability) to use\n",
            "the internet that will usher in the greatest transfer of wealth in human history. WAGMI.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "import textwrap\n",
        "import openai\n",
        "import time\n",
        "\n",
        "def generate_chatgpt_single_response(prompt, row, background, print_out = False):\n",
        "    for i in range(5):  #Try 5 times, else return empty array\n",
        "        try:\n",
        "            chatgpt_prompt = prompt.format(row, background)\n",
        "            if print_out:\n",
        "                print(chatgpt_prompt)\n",
        "                print(\"-\"*50)\n",
        "            completion = openai.ChatCompletion.create(\n",
        "                            model = \"gpt-3.5-turbo\",\n",
        "                            messages = [{\"role\": \"user\", \"content\": chatgpt_prompt}]) # don't accidentally use \"prompt\" here instead\n",
        "\n",
        "            # Process chatgpt response, splitting rows based on the format 1:, 2:, ..., and then stripping them on whitespaces\n",
        "            generated_output = completion.choices[0].message.content\n",
        "            if print_out:\n",
        "                print(textwrap.fill(generated_output, width=100))\n",
        "            return [row, generated_output]\n",
        "        \n",
        "        except Exception as e:  # Adjust the exception type if needed\n",
        "            print(f\"Error encountered: {e}. Retrying in 5 seconds. i={i}.For row: {row}\")\n",
        "            time.sleep(5)\n",
        "    return []"
      ],
      "metadata": {
        "id": "LQrH5ZdH325D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"\"\"Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\", and any relevant details from \"BACKGROUND INFORMATION\" to generate atmosphere:\n",
        "\n",
        "PROMPT:\n",
        "Generate a very brief description about the following character\n",
        "\n",
        "CHARACTER DETAILS:\n",
        "{}\n",
        "\n",
        "BACKGROUND INFORMATION:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\":\n",
        "\n",
        "PROMPT:\n",
        "Generate a very brief description about the following character, ignoring using any background information - not using any of it in the output\n",
        "\n",
        "CHARACTER DETAILS:\n",
        "{}\n",
        "\n",
        "BACKGROUND INFORMATION:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\", and any relevant details from \"BACKGROUND INFORMATION\" to generate atmosphere:\n",
        "\n",
        "PROMPT:\n",
        "Write a very short story about the following character\n",
        "\n",
        "CHARACTER DETAILS:\n",
        "{}\n",
        "\n",
        "BACKGROUND INFORMATION:\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "prompts = [[\"Description with background\", prompt1],\n",
        "           [\"Description without background\", prompt2],\n",
        "           [\"Short story\", prompt3]]\n",
        "prompts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooz4kjTGxgti",
        "outputId": "80e3a561-1225-444e-c13c-c7ddc8ae859a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Description with background',\n",
              "  'Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\", and any relevant details from \"BACKGROUND INFORMATION\" to generate atmosphere:\\n\\nPROMPT:\\nGenerate a very brief description about the following character\\n\\nCHARACTER DETAILS:\\n{}\\n\\nBACKGROUND INFORMATION:\\n{}\\n'],\n",
              " ['Description without background',\n",
              "  'Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\":\\n\\nPROMPT:\\nGenerate a very brief description about the following character, ignoring using any background information - not using any of it in the output\\n\\nCHARACTER DETAILS:\\n{}\\n\\nBACKGROUND INFORMATION:\\n{}\\n'],\n",
              " ['Short story',\n",
              "  'Follow the following \"PROMPT\", utilizing every detail for the character in \"CHARACTER DETAILS\", and any relevant details from \"BACKGROUND INFORMATION\" to generate atmosphere:\\n\\nPROMPT:\\nWrite a very short story about the following character\\n\\nCHARACTER DETAILS:\\n{}\\n\\nBACKGROUND INFORMATION:\\n{}\\n']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Multi-threaded ###\n",
        "import concurrent.futures\n",
        "import textwrap\n",
        "import openai\n",
        "import time\n",
        "\n",
        "def generate_chatgpt_single_response(prompt, row, background, print_out = False):\n",
        "    for i in range(5):  #Try 5 times, else return empty array\n",
        "        try:\n",
        "            chatgpt_prompt = prompt.format(row, background)\n",
        "            if print_out:\n",
        "                print(chatgpt_prompt)\n",
        "                print(\"-\"*50)\n",
        "            completion = openai.ChatCompletion.create(\n",
        "                            model = \"gpt-3.5-turbo\",\n",
        "                            messages = [{\"role\": \"user\", \"content\": chatgpt_prompt}]) # don't accidentally use \"prompt\" here instead\n",
        "\n",
        "            # Process chatgpt response, splitting rows based on the format 1:, 2:, ..., and then stripping them on whitespaces\n",
        "            generated_output = completion.choices[0].message.content\n",
        "            if print_out:\n",
        "                print(textwrap.fill(generated_output, width=100))\n",
        "            return [row, generated_output]\n",
        "        \n",
        "        except Exception as e:  # Adjust the exception type if needed\n",
        "            print(f\"Error encountered: {e}. Retrying in 5 seconds. i={i}.For row: {row}\")\n",
        "            time.sleep(5)\n",
        "    return []\n",
        "\n",
        "\n",
        "all_reponses = []\n",
        "t_start = time.time()\n",
        "for title, prompt in prompts:\n",
        "    print(f'Generating {len(formatted_data)} responses for title: \"{title}\" | time elapsed: {time.time()-t_start:.1f} seconds')\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        responses = list(executor.map(lambda row: generate_chatgpt_single_response(prompt, row, chillennials_background1, print_out=False), formatted_data))\n",
        "   \n",
        "    new_responses = []\n",
        "    for response in responses:\n",
        "        if response:\n",
        "            original_row, generated = response\n",
        "            new_responses.append({\"title\": title, \"row\" : response[0], \"generated\" : response[1]})\n",
        "    print(f\"Generated {len(new_responses)}/{len(formatted_data)} | time elapsed: {time.time()-t_start:.1f} seconds\")\n",
        "    all_reponses.extend(new_responses)\n",
        "\n",
        "with open('generated_prompts.jsonl', 'w') as file:\n",
        "    for row in all_reponses:\n",
        "        json.dump(row, file)\n",
        "        file.write('\\n')"
      ],
      "metadata": {
        "id": "iGeLOjH5fFW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e8a82f-1d2f-4535-ec22-80595c7c1190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating 100 responses for title: \"Description with background\" | time elapsed: 0.0 seconds\n",
            "Generated 100/100 | time elapsed: 120.3 seconds\n",
            "Generating 100 responses for title: \"Description without background\" | time elapsed: 120.3 seconds\n",
            "Generated 100/100 | time elapsed: 181.6 seconds\n",
            "Generating 100 responses for title: \"Short story\" | time elapsed: 181.6 seconds\n",
            "Generated 100/100 | time elapsed: 500.9 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_APIKEY"
      ],
      "metadata": {
        "id": "Fhf9tK9rNvqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### asynchronous ###\n",
        "!pip install nest_asyncio  # Install the library if you haven't\n",
        "\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import textwrap\n",
        "import openai\n",
        "import time\n",
        "import json\n",
        "from aiohttp import ClientSession\n",
        "\n",
        "nest_asyncio.apply()  # Apply nest_asyncio to enable nested usage of asyncio's event loop\n",
        "\n",
        "openai.api_key = OPENAI_APIKEY  # Provide your OpenAI API key here\n",
        "openai.aiosession.set(ClientSession())  # Set OpenAI's aiohttp ClientSession\n",
        "\n",
        "async def generate_chatgpt_single_response(prompt, row, background, print_out=False):\n",
        "    for i in range(5):  #Try 5 times, else return empty array\n",
        "        try:\n",
        "            chatgpt_prompt = prompt.format(row, background)\n",
        "            if print_out:\n",
        "                print(chatgpt_prompt)\n",
        "                print(\"-\"*50)\n",
        "            \n",
        "            completion = await openai.ChatCompletion.acreate(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[{\"role\": \"user\", \"content\": chatgpt_prompt}])\n",
        "\n",
        "            # Process chatgpt response, splitting rows based on the format 1:, 2:, ..., and then stripping them on whitespaces\n",
        "            generated_output = completion.choices[0].message.content\n",
        "            if print_out:\n",
        "                print(textwrap.fill(generated_output, width=100))\n",
        "            return [row, generated_output]\n",
        "\n",
        "        except Exception as e:  # Adjust the exception type if needed\n",
        "            print(f\"Error encountered: {e}. Retrying in 5 seconds. i={i}. For row: {row}\")\n",
        "            await asyncio.sleep(5)\n",
        "    return []\n",
        "\n",
        "all_reponses = []\n",
        "t_start = time.time()\n",
        "for title, prompt in prompts:\n",
        "    print(f'Generating {len(formatted_data)} responses for title: \"{title}\" | time elapsed: {time.time()-t_start:.1f} seconds')\n",
        "\n",
        "    tasks = [generate_chatgpt_single_response(prompt, row, chillennials_background1, print_out=False) for row in formatted_data]\n",
        "    responses = await asyncio.gather(*tasks)\n",
        "\n",
        "    new_responses = []\n",
        "    for response in responses:\n",
        "        if response:\n",
        "            original_row, generated = response\n",
        "            new_responses.append({\"title\": title, \"row\" : response[0], \"generated\" : response[1]})\n",
        "    print(f\"Generated {len(new_responses)}/{len(formatted_data)} | time elapsed: {time.time()-t_start:.1f} seconds\")\n",
        "    all_reponses.extend(new_responses)\n",
        "\n",
        "with open('generated_prompts.jsonl', 'w') as file:\n",
        "    for row in all_reponses:\n",
        "        json.dump(row, file)\n",
        "        file.write('\\n')\n",
        "\n",
        "# At the end of your program, close the http session\n",
        "await openai.aiosession.get().close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYDIdjHMM89q",
        "outputId": "9fdf15ab-653e-4efe-802d-df153218eacf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.5.6)\n",
            "Generating 100 responses for title: \"Description with background\" | time elapsed: 0.0 seconds\n",
            "Generated 100/100 | time elapsed: 11.6 seconds\n",
            "Generating 100 responses for title: \"Description without background\" | time elapsed: 11.6 seconds\n",
            "Generated 100/100 | time elapsed: 17.5 seconds\n",
            "Generating 100 responses for title: \"Short story\" | time elapsed: 17.5 seconds\n",
            "Generated 100/100 | time elapsed: 47.5 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsZM6hrJMZ0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMWYMA91MZ21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Upload dataset file to repository\n",
        "# api.upload_file(\n",
        "#     path_or_fileobj=\"/content/generated_prompts.jsonl\",\n",
        "#     path_in_repo=\"multiple_prompts_v1.jsonl\",\n",
        "#     repo_id=f\"sortxyz/catalog\",\n",
        "#     repo_type=\"dataset\",\n",
        "# )"
      ],
      "metadata": {
        "id": "EDYGSXVwPEdz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c5fcbe34-7ffc-4fb0-8f83-268d12999906"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'https://huggingface.co/datasets/sortxyz/catalog/blob/main/multiple_prompts_v1.jsonl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Running gradio inference to prevent the code from crashing ###\n",
        "# # Given a row index, it check it is valid and then outputs the inputted row and chatgpt generation\n",
        "# def get_row(string_row_idx):\n",
        "#     try:\n",
        "#         row_idx = int(string_row_idx)\n",
        "#     except ValueError:\n",
        "#         return f\"Error: Please input a integer in [0, {len(raw_dataset)})\"\n",
        "\n",
        "#     if row_idx < 0 or row_idx > len(raw_dataset) - 1:\n",
        "#         return f\"Error The number you inputted was not in [0, {len(raw_dataset)})\"\n",
        "        \n",
        "#     return str(raw_dataset[row_idx])\n",
        "\n",
        "# # Start gradio interface which runs indefinitely\n",
        "# g = gr.Interface(fn=get_row,\n",
        "#                  inputs=\"text\",\n",
        "#                  outputs=\"text\",\n",
        "#                  title=config['gradio_title'],\n",
        "#                  description=config['gradio_description'].format(len(raw_dataset), len(codepen_data)))\n",
        "# g.launch()   "
      ],
      "metadata": {
        "id": "rx3ISlY86yrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Example output ##\n",
        "# row: {'id': 985, 'Accessories': 'Brown Aviator Sunglasses', 'Apparel': 'Tangerine Button Up', 'Background': 'Purple', 'Expression': 'Teeth', 'Hair': 'Black Helmet Head', 'Skin': 'Green'}\n",
        "### Generate a very brief description about the following character ###\n",
        "#\n",
        "# A chillennial with black helmet head, green skin, brown aviator sunglasses, and a tangerine button-\n",
        "# up shirt was sitting in a purple room, flashing his teeth with a relaxed expression. Despite being\n",
        "# screwed and broke, he remained chill AF.\n",
        "#\n",
        "# A chillennial with a green skin tone, sporting a black helmet head of hair, a tangerine button-up\n",
        "# and brown aviator sunglasses, shows off his teeth while exuding a calm and collected vibe, despite\n",
        "# financial struggles and the frustration of tech support for his boomer parents.\n",
        "\n",
        "### Generate a very brief description about the following character, only using their characters details, and NO background information ###\n",
        "# A cool and collected individual in a tangerine button-up, black helmet head, and brown aviator\n",
        "# sunglasses. Their teeth shine brightly as they confidently navigate the world around them, all while\n",
        "# sporting a unique shade of green skin and a background of purple.\n",
        "\n",
        "# He strutted down the crowded street, his tangerine button-up shirt contrasting sharply against his\n",
        "# green skin. His black helmet hair barely moved in the light breeze, but his brown aviator sunglasses\n",
        "# glinted in the sunshine. Despite being broke and in debt, he was still one of the cool Chillenials -\n",
        "# always calm and collected, never breaking a sweat. As he passed by, people couldn't help but turn\n",
        "# and stare at his unique appearance. But what really caught their attention was his toothy grin,\n",
        "# flashing white against his green skin. He may not have it all figured out, but at least he could\n",
        "# rock a killer outfit and a chill attitude.\n",
        "\n",
        "# row = formatted_data[0]\n",
        "# chatgpt_prompt = prompt3.format(row, chillennials_background1)\n",
        "# print(chatgpt_prompt)\n",
        "# print(\"-\"*50)\n",
        "# completion = openai.ChatCompletion.create(\n",
        "#                 model = \"gpt-3.5-turbo\",\n",
        "#                 messages = [{\"role\": \"user\", \"content\": chatgpt_prompt}]) # don't accidentally use \"prompt\" here instead\n",
        "\n",
        "# # Process chatgpt response, splitting rows based on the format 1:, 2:, ..., and then stripping them on whitespaces\n",
        "# generated_output = completion.choices[0].message.content\n",
        "# print(textwrap.fill(generated_output, width=100))"
      ],
      "metadata": {
        "id": "9z1YIMnc4zx6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}